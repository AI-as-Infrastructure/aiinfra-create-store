{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os, re, torch, hashlib, uuid, json, redis\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter  # Import other text splitters as needed\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.redis import Redis\n",
    "from langchain.schema import Document  # Import Document abstraction\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Directory paths and metadata\n",
    "corpora = {\n",
    "    \"/home/jamessmithies/Dropbox/Technical/projects/aiinfra/vector_sources/1901/au/hofreps/txt\": \"1901-au\",\n",
    "    \"/home/jamessmithies/Dropbox/Technical/projects/aiinfra/vector_sources/1901/nz/hofreps/txt\": \"1901-nz\",\n",
    "    \"/home/jamessmithies/Dropbox/Technical/projects/aiinfra/vector_sources/1901/uk/hofcoms/txt\": \"1901-uk\"\n",
    "}\n",
    "\n",
    "# Access the EMBEDDING_MODEL environment variable\n",
    "EMBEDDING_MODEL = os.getenv('EMBEDDING_MODEL')\n",
    "\n",
    "# Access the REDIS_URL environment variable\n",
    "REDIS_URL = os.getenv('REDIS_URL')\n",
    "\n",
    "# Access the INDEX_NAME environment variable\n",
    "INDEX_NAME = os.getenv('INDEX_NAME')\n",
    "\n",
    "# Access the TEXT_SPLITTER_TYPE, CHUNK_SIZE, and CHUNK_OVERLAP environment variables\n",
    "TEXT_SPLITTER_TYPE = os.getenv('TEXT_SPLITTER_TYPE', 'RecursiveCharacterTextSplitter')\n",
    "CHUNK_SIZE = int(os.getenv('CHUNK_SIZE', 500))\n",
    "CHUNK_OVERLAP = int(os.getenv('CHUNK_OVERLAP', 75))\n",
    "\n",
    "# Function to initialize the text splitter based on the type\n",
    "def get_text_splitter(splitter_type, chunk_size, chunk_overlap):\n",
    "    if splitter_type == 'RecursiveCharacterTextSplitter':\n",
    "        return RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    elif splitter_type == 'CharacterTextSplitter':\n",
    "        return CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported text splitter type: {splitter_type}\")\n",
    "\n",
    "# Connect to Redis, either locally (if installed) or Redis Cloud (details in .env)\n",
    "redis_client = redis.Redis.from_url(REDIS_URL)\n",
    "\n",
    "# Load a tokenizer and model for embedding\n",
    "tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL)\n",
    "model = AutoModel.from_pretrained(EMBEDDING_MODEL)\n",
    "\n",
    "# Function to compute embeddings using Langchain\n",
    "def compute_embedding(text):\n",
    "    try:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing embedding for text: {text[:50]}... - {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Extract date from filename\n",
    "def extract_date_from_filename(filename):\n",
    "    print(f\"Extracting date from filename: {filename}\")  # Debug print\n",
    "    match = re.search(r'(\\w+, \\d{1,2}(?:st|nd|rd|th)? \\w+, \\d{4})', filename)\n",
    "    if match:\n",
    "        date = match.group(0)\n",
    "        print(f\"Extracted date: {date}\")  # Debug print\n",
    "        return date\n",
    "    print(\"No date found, returning 'Unknown Date'\")  # Debug print\n",
    "    return \"Unknown Date\"\n",
    "\n",
    "# Extract URL from text\n",
    "def extract_url(text):\n",
    "    matches = re.finditer(r'<url>(https?://[^\\s]+)</url>', text)\n",
    "    return [(match.start(), match.group(1)) for match in matches]\n",
    "\n",
    "# Extract page number from text\n",
    "def extract_page_number(text):\n",
    "    matches = re.finditer(r'<page>(\\d+)</page>', text)\n",
    "    return [(match.start(), match.group(1)) for match in matches]\n",
    "\n",
    "# Modify the generate_unique_key function to include corpus metadata\n",
    "def generate_unique_key(base_key, chunk_idx, corpus_metadata):\n",
    "    return f\"{base_key}:{corpus_metadata}:{chunk_idx}\"\n",
    "\n",
    "# Update the process_corpus function to pass the corpus metadata\n",
    "def process_corpus(directory, metadata):\n",
    "    # Load documents using Langchain's DirectoryLoader\n",
    "    loader = DirectoryLoader(directory, glob=\"*.txt\")\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # Initialize the text splitter\n",
    "    text_splitter = get_text_splitter(TEXT_SPLITTER_TYPE, CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "    \n",
    "    chunk_counter = 0\n",
    "    texts = []\n",
    "    metadatas = []\n",
    "    embeddings = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        # Extract metadata from the entire document content\n",
    "        date_info = extract_date_from_filename(doc.metadata['source'])\n",
    "        url_info = extract_url(doc.page_content)\n",
    "        page_info = extract_page_number(doc.page_content)\n",
    "        \n",
    "        # Log the extracted metadata\n",
    "        print(f\"Extracted URL: {url_info} from document: {doc.metadata['source']}\")\n",
    "        print(f\"Extracted Page Numbers: {page_info} from document: {doc.metadata['source']}\")\n",
    "        \n",
    "        # Check if there is a URL at the top of the document\n",
    "        top_url = url_info[0][1] if url_info else None\n",
    "        \n",
    "        # Check if there are any page tags\n",
    "        if not page_info:\n",
    "            # Process the entire document as a single section\n",
    "            current_url = top_url\n",
    "            clean_section = re.sub(r'<url>https?://[^\\s]+</url>', '', doc.page_content)\n",
    "            chunked_texts = text_splitter.split_text(clean_section)\n",
    "            for chunk_idx, chunk in enumerate(chunked_texts):\n",
    "                embedding = compute_embedding(chunk)\n",
    "                if embedding is not None:\n",
    "                    # Generate a unique key for each chunk\n",
    "                    redis_key = generate_unique_key(f\"doc:{INDEX_NAME}\", chunk_counter, metadata)\n",
    "                    chunk_counter += 1\n",
    "                    \n",
    "                    # Create metadata in the required format\n",
    "                    metadata_dict = {\n",
    "                        \"source\": doc.metadata['source'],\n",
    "                        \"date\": date_info,\n",
    "                        \"url\": current_url,\n",
    "                        \"page\": None,\n",
    "                        \"loc\": json.dumps({\n",
    "                            \"lines\": {\n",
    "                                \"from\": chunk_idx * (CHUNK_SIZE - CHUNK_OVERLAP) + 1,\n",
    "                                \"to\": (chunk_idx + 1) * CHUNK_SIZE\n",
    "                            }\n",
    "                        }),\n",
    "                        \"corpus\": metadata \n",
    "                    }\n",
    "                    \n",
    "                    # Append to lists for batch processing\n",
    "                    texts.append(chunk)\n",
    "                    metadatas.append(metadata_dict)\n",
    "                    embeddings.append(embedding.tolist())\n",
    "        else:\n",
    "            # Split the document into sections based on <page> tags\n",
    "            sections = re.split(r'(<page>\\d+</page>)', doc.page_content)\n",
    "            \n",
    "            current_page = None\n",
    "            current_url = None\n",
    "            for section in sections:\n",
    "                if section.startswith('<page>'):\n",
    "                    current_page = int(re.search(r'<page>(\\d+)</page>', section).group(1))\n",
    "                    # If there is no URL under page tags, use the top URL\n",
    "                    if not any(url for pos, url in url_info if pos > section.find('<page>')):\n",
    "                        current_url = top_url\n",
    "                else:\n",
    "                    # Update the current URL if the section contains a new URL tag\n",
    "                    for pos, url in url_info:\n",
    "                        if section.find(url) != -1:\n",
    "                            current_url = url\n",
    "                            break\n",
    "                    \n",
    "                    if current_page is not None:\n",
    "                        # Remove URL tags from the section\n",
    "                        clean_section = re.sub(r'<url>https?://[^\\s]+</url>', '', section)\n",
    "                        \n",
    "                        chunked_texts = text_splitter.split_text(clean_section)\n",
    "                        for chunk_idx, chunk in enumerate(chunked_texts):\n",
    "                            embedding = compute_embedding(chunk)\n",
    "                            if embedding is not None:\n",
    "                                # Generate a unique key for each chunk\n",
    "                                redis_key = generate_unique_key(f\"doc:{INDEX_NAME}\", chunk_counter, metadata)\n",
    "                                chunk_counter += 1\n",
    "                                \n",
    "                                # Create metadata in the required format\n",
    "                                metadata_dict = {\n",
    "                                    \"source\": doc.metadata['source'],\n",
    "                                    \"date\": date_info,\n",
    "                                    \"url\": current_url,\n",
    "                                    \"page\": current_page,\n",
    "                                    \"loc\": json.dumps({\n",
    "                                        \"lines\": {\n",
    "                                            \"from\": chunk_idx * (CHUNK_SIZE - CHUNK_OVERLAP) + 1,\n",
    "                                            \"to\": (chunk_idx + 1) * CHUNK_SIZE\n",
    "                                        }\n",
    "                                    }),\n",
    "                                    \"corpus\": metadata \n",
    "                                }\n",
    "                                \n",
    "                                # Append to lists for batch processing\n",
    "                                texts.append(chunk)\n",
    "                                metadatas.append(metadata_dict)\n",
    "                                embeddings.append(embedding.tolist())\n",
    "\n",
    "    # Add texts to the vector store to create the index\n",
    "    vector_store.add_texts(texts, metadatas=metadatas, embeddings=embeddings)\n",
    "\n",
    "    print(f\"Finished processing corpus: {metadata}\")\n",
    "\n",
    "# Initialize HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "\n",
    "# Initialize Redis vector store\n",
    "vector_store = Redis(\n",
    "    redis_url=REDIS_URL,\n",
    "    embedding=embeddings,\n",
    "    index_name=INDEX_NAME,\n",
    ")\n",
    "\n",
    "# Process each corpus\n",
    "for directory, metadata in corpora.items():\n",
    "    process_corpus(directory, metadata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
