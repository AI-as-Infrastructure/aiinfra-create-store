{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jamessmithies/Dropbox/Technical/projects/aiinfra/aiinfra-redis-pipeline/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/home/jamessmithies/Dropbox/Technical/projects/aiinfra/aiinfra-redis-pipeline/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting date from filename: /home/jamessmithies/Dropbox/Technical/projects/aiinfra/vector_sources/1901/au/hofreps/txt/Friday, 04 October, 1901.txt\n",
      "Extracted date: Friday, 04 October, 1901\n",
      "Extracted URL: [(0, 'https://www.historichansard.net/hofreps/1901/19011004_reps_1_5')] from document: /home/jamessmithies/Dropbox/Technical/projects/aiinfra/vector_sources/1901/au/hofreps/txt/Friday, 04 October, 1901.txt\n",
      "Extracted Page Numbers: [(1407, '5641'), (3973, '5642'), (10707, '5643'), (15084, '5644'), (25357, '5646'), (28235, '5647'), (33064, '5648'), (36174, '5649'), (43448, '5650'), (47734, '5651'), (52095, '5652'), (56561, '5653'), (60595, '5654'), (64613, '5655'), (68094, '5656'), (73448, '5657'), (76685, '5658'), (88427, '5660'), (95893, '5662'), (100468, '5663'), (105210, '5664'), (108957, '5665'), (114420, '5666'), (118633, '5667'), (122730, '5668')] from document: /home/jamessmithies/Dropbox/Technical/projects/aiinfra/vector_sources/1901/au/hofreps/txt/Friday, 04 October, 1901.txt\n",
      "Extracting date from filename: /home/jamessmithies/Dropbox/Technical/projects/aiinfra/vector_sources/1901/au/hofreps/txt/Friday, 02 August, 1901.txt\n",
      "Extracted date: Friday, 02 August, 1901\n",
      "Extracted URL: [(0, 'https://www.historichansard.net/hofreps/1901/19010802_reps_1_3')] from document: /home/jamessmithies/Dropbox/Technical/projects/aiinfra/vector_sources/1901/au/hofreps/txt/Friday, 02 August, 1901.txt\n",
      "Extracted Page Numbers: [(2147, '3473'), (6888, '3474'), (11042, '3475'), (14004, '3476'), (18388, '3477'), (23566, '3478'), (28268, '3479'), (31235, '3480'), (39555, '3481'), (44240, '3482'), (48925, '3483'), (53148, '3484'), (58234, '3485'), (62699, '3486'), (66769, '3487'), (71576, '3488'), (76225, '3489'), (77971, '3490')] from document: /home/jamessmithies/Dropbox/Technical/projects/aiinfra/vector_sources/1901/au/hofreps/txt/Friday, 02 August, 1901.txt\n",
      "Finished processing corpus: 1901-au\n",
      "Extracting date from filename: /home/jamessmithies/Dropbox/Technical/projects/aiinfra/vector_sources/1901/nz/hofreps/txt/Friday, 5th July, 1901.txt\n",
      "Extracted date: Friday, 5th July, 1901\n",
      "Extracted URL: [(2429, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=131'), (8286, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=132'), (14697, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=133'), (20848, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=134'), (27097, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=135'), (33397, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=136'), (39643, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=137'), (45683, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=138'), (51694, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=139'), (58023, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=140'), (64447, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=141'), (70816, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=142'), (77143, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=143'), (83527, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=144'), (89947, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=145'), (96326, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=146'), (102656, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=147'), (109050, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=148'), (115439, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=149'), (121862, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=150'), (128108, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=151'), (134382, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=152'), (140699, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=153'), (146927, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=154'), (153271, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=155'), (159623, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=156'), (166033, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=157'), (172407, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=158'), (178744, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=159'), (184977, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=160'), (191260, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=161'), (197431, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=162'), (203581, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=163'), (209566, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=164'), (215954, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=165'), (222334, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=166'), (228594, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=167'), (234919, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=168'), (239190, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=169'), (245566, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=170'), (248686, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=171'), (254409, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=172'), (260680, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=173'), (266739, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=174'), (272815, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=175'), (279124, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=176'), (285133, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=177'), (291480, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=178'), (297963, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=179'), (304257, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=180'), (310497, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=181'), (316984, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=182'), (323358, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=183'), (329703, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=184'), (336004, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=185'), (342282, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=186'), (348726, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=187'), (355035, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=188'), (361372, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=189'), (367668, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=190'), (373007, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=191'), (379181, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=192'), (385200, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=193'), (391566, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=194'), (397985, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=195'), (404325, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=196'), (410658, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=197'), (417031, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=198'), (423343, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=199'), (429624, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=200'), (435919, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=201')] from document: /home/jamessmithies/Dropbox/Technical/projects/aiinfra/vector_sources/1901/nz/hofreps/txt/Friday, 5th July, 1901.txt\n",
      "Extracted Page Numbers: [(2411, '103'), (8268, '104'), (14679, '105'), (20830, '106'), (27079, '107'), (33379, '108'), (39625, '109'), (45665, '110'), (51676, '111'), (58005, '112'), (64429, '113'), (70798, '114'), (77125, '115'), (83509, '116'), (89929, '117'), (96308, '118'), (102638, '119'), (109032, '120'), (115421, '121'), (121844, '122'), (128090, '123'), (134364, '124'), (140681, '125'), (146909, '126'), (153253, '127'), (159605, '128'), (166015, '129'), (172389, '130'), (178726, '131'), (184959, '132'), (191242, '133'), (197413, '134'), (203563, '135'), (209548, '136'), (215936, '137'), (222316, '138'), (228576, '139'), (234901, '140'), (239172, '141'), (245548, '142'), (248668, '143'), (254391, '144'), (260662, '145'), (266721, '146'), (272797, '147'), (279106, '148'), (285115, '149'), (291462, '150'), (297945, '151'), (304239, '152'), (310479, '153'), (316966, '154'), (323340, '155'), (329685, '156'), (335986, '157'), (342264, '158'), (348708, '159'), (355017, '160'), (361354, '161'), (367650, '162'), (372989, '163'), (379163, '164'), (385182, '165'), (391548, '166'), (397967, '167'), (404307, '168'), (410640, '169'), (417013, '170'), (423325, '171'), (429606, '172'), (435901, '173')] from document: /home/jamessmithies/Dropbox/Technical/projects/aiinfra/vector_sources/1901/nz/hofreps/txt/Friday, 5th July, 1901.txt\n",
      "Extracting date from filename: /home/jamessmithies/Dropbox/Technical/projects/aiinfra/vector_sources/1901/nz/hofreps/txt/Friday, 04 October, 1901.txt\n",
      "Extracted date: Friday, 04 October, 1901\n",
      "Extracted URL: [(0, 'https://www.historichansard.net/hofreps/1901/19011004_reps_1_5')] from document: /home/jamessmithies/Dropbox/Technical/projects/aiinfra/vector_sources/1901/nz/hofreps/txt/Friday, 04 October, 1901.txt\n",
      "Extracted Page Numbers: [(1407, '5641'), (3973, '5642'), (10707, '5643'), (15084, '5644'), (25357, '5646'), (28235, '5647'), (33064, '5648'), (36174, '5649'), (43448, '5650'), (47734, '5651'), (52095, '5652'), (56561, '5653'), (60595, '5654'), (64613, '5655'), (68094, '5656'), (73448, '5657'), (76685, '5658'), (88427, '5660'), (95893, '5662'), (100468, '5663'), (105210, '5664'), (108957, '5665'), (114420, '5666'), (118633, '5667'), (122730, '5668')] from document: /home/jamessmithies/Dropbox/Technical/projects/aiinfra/vector_sources/1901/nz/hofreps/txt/Friday, 04 October, 1901.txt\n",
      "Extracting date from filename: /home/jamessmithies/Dropbox/Technical/projects/aiinfra/vector_sources/1901/nz/hofreps/txt/Friday, 12th July, 1901.txt\n",
      "Extracted date: Friday, 12th July, 1901\n",
      "Extracted URL: [(313, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=354'), (6100, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=355'), (12291, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=356'), (18182, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=357'), (24451, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=358'), (30541, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=359'), (36785, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=360'), (43029, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=361'), (48976, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=362'), (55361, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=363'), (61641, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=364'), (67930, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=365'), (74000, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=366'), (80422, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=367'), (86497, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=368'), (92758, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=369'), (98844, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=370'), (105192, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=371'), (111362, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=372'), (117522, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=373'), (123790, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=374'), (130016, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=375'), (136199, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=376'), (142435, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=377'), (148810, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=378'), (155232, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=379'), (161566, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=380'), (167806, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=381'), (174005, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=382'), (180511, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=383'), (186497, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=384'), (192649, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=385'), (198844, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=386'), (204825, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=387'), (210612, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=388'), (216516, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=389'), (222632, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=390'), (228743, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=391'), (233436, 'https://babel.hathitrust.org/cgi/pt?id=uc1.32106019788238&seq=392')] from document: /home/jamessmithies/Dropbox/Technical/projects/aiinfra/vector_sources/1901/nz/hofreps/txt/Friday, 12th July, 1901.txt\n",
      "Extracted Page Numbers: [(295, '326'), (6082, '327'), (12273, '328'), (18164, '329'), (24433, '330'), (30523, '331'), (36767, '332'), (43011, '333'), (48958, '334'), (55343, '335'), (61623, '336'), (67912, '337'), (73982, '338'), (80404, '339'), (86479, '340'), (92740, '341'), (98826, '342'), (105174, '343'), (111344, '344'), (117504, '345'), (123772, '346'), (129998, '347'), (136181, '348'), (142417, '349'), (148792, '350'), (155214, '351'), (161548, '352'), (167788, '353'), (173987, '354'), (180493, '355'), (186479, '356'), (192631, '357'), (198826, '358'), (204807, '359'), (210594, '360'), (216498, '361'), (222614, '362'), (228725, '363'), (233418, '364')] from document: /home/jamessmithies/Dropbox/Technical/projects/aiinfra/vector_sources/1901/nz/hofreps/txt/Friday, 12th July, 1901.txt\n",
      "Extracting date from filename: /home/jamessmithies/Dropbox/Technical/projects/aiinfra/vector_sources/1901/nz/hofreps/txt/Friday, 02 August, 1901.txt\n",
      "Extracted date: Friday, 02 August, 1901\n",
      "Extracted URL: [(0, 'https://www.historichansard.net/hofreps/1901/19010802_reps_1_3')] from document: /home/jamessmithies/Dropbox/Technical/projects/aiinfra/vector_sources/1901/nz/hofreps/txt/Friday, 02 August, 1901.txt\n",
      "Extracted Page Numbers: [(2147, '3473'), (6888, '3474'), (11042, '3475'), (14004, '3476'), (18388, '3477'), (23566, '3478'), (28268, '3479'), (31235, '3480'), (39555, '3481'), (44240, '3482'), (48925, '3483'), (53148, '3484'), (58234, '3485'), (62699, '3486'), (66769, '3487'), (71576, '3488'), (76225, '3489'), (77971, '3490')] from document: /home/jamessmithies/Dropbox/Technical/projects/aiinfra/vector_sources/1901/nz/hofreps/txt/Friday, 02 August, 1901.txt\n",
      "Finished processing corpus: 1901-nz\n",
      "Extracting date from filename: /home/jamessmithies/Dropbox/Technical/projects/aiinfra/vector_sources/1901/uk/hofcoms/txt/Friday, 15th February, 1901.txt\n",
      "Extracted date: Friday, 15th February, 1901\n",
      "Extracted URL: [(0, 'https://api.parliament.uk/historic-hansard/sittings/1901/feb/15')] from document: /home/jamessmithies/Dropbox/Technical/projects/aiinfra/vector_sources/1901/uk/hofcoms/txt/Friday, 15th February, 1901.txt\n",
      "Extracted Page Numbers: [] from document: /home/jamessmithies/Dropbox/Technical/projects/aiinfra/vector_sources/1901/uk/hofcoms/txt/Friday, 15th February, 1901.txt\n",
      "Extracting date from filename: /home/jamessmithies/Dropbox/Technical/projects/aiinfra/vector_sources/1901/uk/hofcoms/txt/Friday, 7th June, 1901.txt\n",
      "Extracted date: Friday, 7th June, 1901\n",
      "Extracted URL: [(0, 'https://api.parliament.uk/historic-hansard/sittings/1901/jun/7')] from document: /home/jamessmithies/Dropbox/Technical/projects/aiinfra/vector_sources/1901/uk/hofcoms/txt/Friday, 7th June, 1901.txt\n",
      "Extracted Page Numbers: [] from document: /home/jamessmithies/Dropbox/Technical/projects/aiinfra/vector_sources/1901/uk/hofcoms/txt/Friday, 7th June, 1901.txt\n",
      "Finished processing corpus: 1901-uk\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os, re, torch, hashlib, uuid, json, redis\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter  # Import other text splitters as needed\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.redis import Redis\n",
    "from langchain.schema import Document  # Import Document abstraction\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Directory paths and metadata\n",
    "corpora = {\n",
    "    \"/home/jamessmithies/Dropbox/Technical/projects/aiinfra/vector_sources/1901/au/hofreps/txt\": \"1901-au\",\n",
    "    \"/home/jamessmithies/Dropbox/Technical/projects/aiinfra/vector_sources/1901/nz/hofreps/txt\": \"1901-nz\",\n",
    "    \"/home/jamessmithies/Dropbox/Technical/projects/aiinfra/vector_sources/1901/uk/hofcoms/txt\": \"1901-uk\"\n",
    "}\n",
    "\n",
    "# Access the EMBEDDING_MODEL environment variable\n",
    "EMBEDDING_MODEL = os.getenv('EMBEDDING_MODEL')\n",
    "\n",
    "# Access the REDIS_URL environment variable\n",
    "REDIS_URL = os.getenv('REDIS_URL')\n",
    "\n",
    "# Access the INDEX_NAME environment variable\n",
    "INDEX_NAME = os.getenv('INDEX_NAME')\n",
    "\n",
    "# Access the TEXT_SPLITTER_TYPE, CHUNK_SIZE, and CHUNK_OVERLAP environment variables\n",
    "TEXT_SPLITTER_TYPE = os.getenv('TEXT_SPLITTER_TYPE', 'RecursiveCharacterTextSplitter')\n",
    "CHUNK_SIZE = int(os.getenv('CHUNK_SIZE', 500))\n",
    "CHUNK_OVERLAP = int(os.getenv('CHUNK_OVERLAP', 75))\n",
    "\n",
    "# Function to initialize the text splitter based on the type\n",
    "def get_text_splitter(splitter_type, chunk_size, chunk_overlap):\n",
    "    if splitter_type == 'RecursiveCharacterTextSplitter':\n",
    "        return RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    elif splitter_type == 'CharacterTextSplitter':\n",
    "        return CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported text splitter type: {splitter_type}\")\n",
    "\n",
    "# Connect to Redis, either locally (if installed) or Redis Cloud (details in .env)\n",
    "redis_client = redis.Redis.from_url(REDIS_URL)\n",
    "\n",
    "# Load a tokenizer and model for embedding\n",
    "tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL)\n",
    "model = AutoModel.from_pretrained(EMBEDDING_MODEL)\n",
    "\n",
    "# Function to compute embeddings using Langchain\n",
    "def compute_embedding(text):\n",
    "    try:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing embedding for text: {text[:50]}... - {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Extract date from filename\n",
    "def extract_date_from_filename(filename):\n",
    "    print(f\"Extracting date from filename: {filename}\")  # Debug print\n",
    "    match = re.search(r'(\\w+, \\d{1,2}(?:st|nd|rd|th)? \\w+, \\d{4})', filename)\n",
    "    if match:\n",
    "        date = match.group(0)\n",
    "        print(f\"Extracted date: {date}\")  # Debug print\n",
    "        return date\n",
    "    print(\"No date found, returning 'Unknown Date'\")  # Debug print\n",
    "    return \"Unknown Date\"\n",
    "\n",
    "# Extract URL from text\n",
    "def extract_url(text):\n",
    "    matches = re.finditer(r'<url>(https?://[^\\s]+)</url>', text)\n",
    "    return [(match.start(), match.group(1)) for match in matches]\n",
    "\n",
    "# Extract page number from text\n",
    "def extract_page_number(text):\n",
    "    matches = re.finditer(r'<page>(\\d+)</page>', text)\n",
    "    return [(match.start(), match.group(1)) for match in matches]\n",
    "\n",
    "# Modify the generate_unique_key function to include corpus metadata\n",
    "def generate_unique_key(base_key, chunk_idx, corpus_metadata):\n",
    "    return f\"{base_key}:{corpus_metadata}:{chunk_idx}\"\n",
    "\n",
    "# Update the process_corpus function to pass the corpus metadata\n",
    "def process_corpus(directory, metadata):\n",
    "    # Load documents using Langchain's DirectoryLoader\n",
    "    loader = DirectoryLoader(directory, glob=\"*.txt\")\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # Initialize the text splitter\n",
    "    text_splitter = get_text_splitter(TEXT_SPLITTER_TYPE, CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "    \n",
    "    chunk_counter = 0\n",
    "    texts = []\n",
    "    metadatas = []\n",
    "    embeddings = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        # Extract metadata from the entire document content\n",
    "        date_info = extract_date_from_filename(doc.metadata['source'])\n",
    "        url_info = extract_url(doc.page_content)\n",
    "        page_info = extract_page_number(doc.page_content)\n",
    "        \n",
    "        # Log the extracted metadata\n",
    "        print(f\"Extracted URL: {url_info} from document: {doc.metadata['source']}\")\n",
    "        print(f\"Extracted Page Numbers: {page_info} from document: {doc.metadata['source']}\")\n",
    "        \n",
    "        # Check if there is a URL at the top of the document\n",
    "        top_url = url_info[0][1] if url_info else None\n",
    "        \n",
    "        # Check if there are any page tags\n",
    "        if not page_info:\n",
    "            # Process the entire document as a single section\n",
    "            current_url = top_url\n",
    "            clean_section = re.sub(r'<url>https?://[^\\s]+</url>', '', doc.page_content)\n",
    "            chunked_texts = text_splitter.split_text(clean_section)\n",
    "            for chunk_idx, chunk in enumerate(chunked_texts):\n",
    "                embedding = compute_embedding(chunk)\n",
    "                if embedding is not None:\n",
    "                    # Generate a unique key for each chunk\n",
    "                    redis_key = generate_unique_key(f\"doc:{INDEX_NAME}\", chunk_counter, metadata)\n",
    "                    chunk_counter += 1\n",
    "                    \n",
    "                    # Create metadata in the required format\n",
    "                    metadata_dict = {\n",
    "                        \"source\": doc.metadata['source'],\n",
    "                        \"date\": date_info,\n",
    "                        \"url\": current_url,\n",
    "                        \"page\": None,\n",
    "                        \"loc\": json.dumps({\n",
    "                            \"lines\": {\n",
    "                                \"from\": chunk_idx * (CHUNK_SIZE - CHUNK_OVERLAP) + 1,\n",
    "                                \"to\": (chunk_idx + 1) * CHUNK_SIZE\n",
    "                            }\n",
    "                        }),\n",
    "                        \"corpus\": metadata \n",
    "                    }\n",
    "                    \n",
    "                    # Append to lists for batch processing\n",
    "                    texts.append(chunk)\n",
    "                    metadatas.append(metadata_dict)\n",
    "                    embeddings.append(embedding.tolist())\n",
    "        else:\n",
    "            # Split the document into sections based on <page> tags\n",
    "            sections = re.split(r'(<page>\\d+</page>)', doc.page_content)\n",
    "            \n",
    "            current_page = None\n",
    "            current_url = None\n",
    "            for section in sections:\n",
    "                if section.startswith('<page>'):\n",
    "                    current_page = int(re.search(r'<page>(\\d+)</page>', section).group(1))\n",
    "                    # If there is no URL under page tags, use the top URL\n",
    "                    if not any(url for pos, url in url_info if pos > section.find('<page>')):\n",
    "                        current_url = top_url\n",
    "                else:\n",
    "                    # Update the current URL if the section contains a new URL tag\n",
    "                    for pos, url in url_info:\n",
    "                        if section.find(url) != -1:\n",
    "                            current_url = url\n",
    "                            break\n",
    "                    \n",
    "                    if current_page is not None:\n",
    "                        # Remove URL tags from the section\n",
    "                        clean_section = re.sub(r'<url>https?://[^\\s]+</url>', '', section)\n",
    "                        \n",
    "                        chunked_texts = text_splitter.split_text(clean_section)\n",
    "                        for chunk_idx, chunk in enumerate(chunked_texts):\n",
    "                            embedding = compute_embedding(chunk)\n",
    "                            if embedding is not None:\n",
    "                                # Generate a unique key for each chunk\n",
    "                                redis_key = generate_unique_key(f\"doc:{INDEX_NAME}\", chunk_counter, metadata)\n",
    "                                chunk_counter += 1\n",
    "                                \n",
    "                                # Create metadata in the required format\n",
    "                                metadata_dict = {\n",
    "                                    \"source\": doc.metadata['source'],\n",
    "                                    \"date\": date_info,\n",
    "                                    \"url\": current_url,\n",
    "                                    \"page\": current_page,\n",
    "                                    \"loc\": json.dumps({\n",
    "                                        \"lines\": {\n",
    "                                            \"from\": chunk_idx * (CHUNK_SIZE - CHUNK_OVERLAP) + 1,\n",
    "                                            \"to\": (chunk_idx + 1) * CHUNK_SIZE\n",
    "                                        }\n",
    "                                    }),\n",
    "                                    \"corpus\": metadata \n",
    "                                }\n",
    "                                \n",
    "                                # Append to lists for batch processing\n",
    "                                texts.append(chunk)\n",
    "                                metadatas.append(metadata_dict)\n",
    "                                embeddings.append(embedding.tolist())\n",
    "\n",
    "    # Add texts to the vector store to create the index\n",
    "    vector_store.add_texts(texts, metadatas=metadatas, embeddings=embeddings)\n",
    "\n",
    "    print(f\"Finished processing corpus: {metadata}\")\n",
    "\n",
    "# Initialize HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "\n",
    "# Initialize Redis vector store\n",
    "vector_store = Redis(\n",
    "    redis_url=REDIS_URL,\n",
    "    embedding=embeddings,\n",
    "    index_name=INDEX_NAME,\n",
    ")\n",
    "\n",
    "# Process each corpus\n",
    "for directory, metadata in corpora.items():\n",
    "    process_corpus(directory, metadata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
